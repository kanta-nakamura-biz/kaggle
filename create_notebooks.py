#!/usr/bin/env python3
"""
Jupyter notebook templates creation script
"""

import nbformat as nbf
import os

def create_eda_notebook():
    """EDAãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ"""
    nb = nbf.v4.new_notebook()
    nb.cells = [
        nbf.v4.new_markdown_cell('# æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ (EDA) ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Kaggleç«¶æŠ€ã«ãŠã‘ã‚‹æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚'),
        nbf.v4.new_markdown_cell('## 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ'),
        nbf.v4.new_code_cell('import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings("ignore")\n\n# è¨­å®š\nplt.rcParams["figure.figsize"] = (12, 8)\npd.set_option("display.max_columns", None)'),
        nbf.v4.new_markdown_cell('## 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿'),
        nbf.v4.new_code_cell('# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\ntrain = pd.read_csv("../data/raw/train.csv")\ntest = pd.read_csv("../data/raw/test.csv")\n\nprint(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train.shape}")\nprint(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test.shape}")'),
        nbf.v4.new_markdown_cell('## 3. åŸºæœ¬æƒ…å ±ã®ç¢ºèª'),
        nbf.v4.new_code_cell('# åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º\nprint("=== è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ± ===")\nprint(train.info())\nprint("\\n=== è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ± ===")\nprint(train.describe())'),
        nbf.v4.new_markdown_cell('## 4. æ¬ æå€¤ã®ç¢ºèª'),
        nbf.v4.new_code_cell('# æ¬ æå€¤ã®ç¢ºèª\nprint("=== è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ ===")\nprint(train.isnull().sum())\nprint("\\n=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ ===")\nprint(test.isnull().sum())'),
        nbf.v4.new_markdown_cell('## 5. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†æ'),
        nbf.v4.new_code_cell('# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒ\nif "target" in train.columns:\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    train["target"].hist(bins=50)\n    plt.title("Target Distribution")\n    plt.subplot(1, 2, 2)\n    train["target"].plot(kind="box")\n    plt.title("Target Box Plot")\n    plt.tight_layout()\n    plt.show()'),
        nbf.v4.new_markdown_cell('## 6. ç‰¹å¾´é‡ã®åˆ†æ'),
        nbf.v4.new_code_cell('# æ•°å€¤ç‰¹å¾´é‡ã®ç›¸é–¢è¡Œåˆ—\nnumeric_cols = train.select_dtypes(include=[np.number]).columns\nif len(numeric_cols) > 1:\n    plt.figure(figsize=(12, 10))\n    correlation_matrix = train[numeric_cols].corr()\n    sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", center=0)\n    plt.title("Feature Correlation Matrix")\n    plt.show()'),
        nbf.v4.new_markdown_cell('## 7. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®åˆ†æ'),
        nbf.v4.new_code_cell('# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ç¢ºèª\ncategorical_cols = train.select_dtypes(include=["object"]).columns\nprint(f"ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°: {list(categorical_cols)}")\n\nfor col in categorical_cols[:5]:  # æœ€åˆã®5ã¤ã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°\n    print(f"\\n=== {col} ===")\n    print(train[col].value_counts().head(10))')
    ]
    
    os.makedirs('shared/notebooks', exist_ok=True)
    with open('shared/notebooks/01_EDA_template.ipynb', 'w') as f:
        nbf.write(nb, f)

def create_feature_engineering_notebook():
    """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ"""
    nb = nbf.v4.new_notebook()
    nb.cells = [
        nbf.v4.new_markdown_cell('# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Kaggleç«¶æŠ€ã«ãŠã‘ã‚‹ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚'),
        nbf.v4.new_markdown_cell('## 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ'),
        nbf.v4.new_code_cell('import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings("ignore")'),
        nbf.v4.new_markdown_cell('## 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿'),
        nbf.v4.new_code_cell('# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\ntrain = pd.read_csv("../data/raw/train.csv")\ntest = pd.read_csv("../data/raw/test.csv")\n\nprint(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train.shape}")\nprint(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test.shape}")'),
        nbf.v4.new_markdown_cell('## 3. æ¬ æå€¤ã®å‡¦ç†'),
        nbf.v4.new_code_cell('# æ¬ æå€¤ã®å‡¦ç†\n# æ•°å€¤å¤‰æ•°: å¹³å‡å€¤ã§è£œå®Œ\nnumeric_cols = train.select_dtypes(include=[np.number]).columns\nfor col in numeric_cols:\n    if train[col].isnull().sum() > 0:\n        mean_val = train[col].mean()\n        train[col].fillna(mean_val, inplace=True)\n        test[col].fillna(mean_val, inplace=True)\n\n# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°: æœ€é »å€¤ã§è£œå®Œ\ncategorical_cols = train.select_dtypes(include=["object"]).columns\nfor col in categorical_cols:\n    if train[col].isnull().sum() > 0:\n        mode_val = train[col].mode()[0]\n        train[col].fillna(mode_val, inplace=True)\n        test[col].fillna(mode_val, inplace=True)'),
        nbf.v4.new_markdown_cell('## 4. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°'),
        nbf.v4.new_code_cell('# ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\nfrom sklearn.preprocessing import LabelEncoder\n\nle_dict = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n    combined = pd.concat([train[col], test[col]], axis=0)\n    le.fit(combined)\n    train[col + "_encoded"] = le.transform(train[col])\n    test[col + "_encoded"] = le.transform(test[col])\n    le_dict[col] = le\n    \nprint("ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†")'),
        nbf.v4.new_markdown_cell('## 5. æ•°å€¤å¤‰æ•°ã®å¤‰æ›'),
        nbf.v4.new_code_cell('# å¯¾æ•°å¤‰æ›ï¼ˆæ­£ã®å€¤ã®ã¿ï¼‰\nfor col in numeric_cols:\n    if (train[col] > 0).all():\n        train[col + "_log"] = np.log1p(train[col])\n        test[col + "_log"] = np.log1p(test[col])\n\n# æ¨™æº–åŒ–\nscaler = StandardScaler()\nscaled_cols = [col for col in numeric_cols if col != "target"]\nif scaled_cols:\n    train_scaled = scaler.fit_transform(train[scaled_cols])\n    test_scaled = scaler.transform(test[scaled_cols])\n    \n    for i, col in enumerate(scaled_cols):\n        train[col + "_scaled"] = train_scaled[:, i]\n        test[col + "_scaled"] = test_scaled[:, i]'),
        nbf.v4.new_markdown_cell('## 6. æ–°ã—ã„ç‰¹å¾´é‡ã®ä½œæˆ'),
        nbf.v4.new_code_cell('# ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›\n# ä¾‹: æ•°å€¤ç‰¹å¾´é‡åŒå£«ã®å››å‰‡æ¼”ç®—\nif len(numeric_cols) >= 2:\n    col1, col2 = numeric_cols[0], numeric_cols[1]\n    train[f"{col1}_plus_{col2}"] = train[col1] + train[col2]\n    train[f"{col1}_minus_{col2}"] = train[col1] - train[col2]\n    train[f"{col1}_mult_{col2}"] = train[col1] * train[col2]\n    train[f"{col1}_div_{col2}"] = train[col1] / (train[col2] + 1e-8)\n    \n    test[f"{col1}_plus_{col2}"] = test[col1] + test[col2]\n    test[f"{col1}_minus_{col2}"] = test[col1] - test[col2]\n    test[f"{col1}_mult_{col2}"] = test[col1] * test[col2]\n    test[f"{col1}_div_{col2}"] = test[col1] / (test[col2] + 1e-8)'),
        nbf.v4.new_markdown_cell('## 7. å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜'),
        nbf.v4.new_code_cell('# å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\nos.makedirs("../data/processed", exist_ok=True)\n\n# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†é›¢\nif "target" in train.columns:\n    X_train = train.drop("target", axis=1)\n    y_train = train["target"]\nelse:\n    X_train = train\n    y_train = None\n\nX_test = test\n\n# ä¿å­˜\nX_train.to_csv("../data/processed/X_train_processed.csv", index=False)\nX_test.to_csv("../data/processed/X_test_processed.csv", index=False)\nif y_train is not None:\n    y_train.to_csv("../data/processed/y_train.csv", index=False)\n\nprint("å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ")\nprint(f"X_trainå½¢çŠ¶: {X_train.shape}")\nprint(f"X_testå½¢çŠ¶: {X_test.shape}")\nif y_train is not None:\n    print(f"y_trainå½¢çŠ¶: {y_train.shape}")')
    ]
    
    with open('shared/notebooks/02_feature_engineering_template.ipynb', 'w') as f:
        nbf.write(nb, f)

def create_modeling_notebook():
    """ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ"""
    nb = nbf.v4.new_notebook()
    nb.cells = [
        nbf.v4.new_markdown_cell('# ãƒ¢ãƒ‡ãƒªãƒ³ã‚° ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Kaggleç«¶æŠ€ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚'),
        nbf.v4.new_markdown_cell('## 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ'),
        nbf.v4.new_code_cell('import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\nimport optuna\nimport joblib\nimport warnings\nwarnings.filterwarnings("ignore")\n\n# è¨­å®š\nplt.rcParams["figure.figsize"] = (12, 8)\npd.set_option("display.max_columns", None)\n\n# ã‚·ãƒ¼ãƒ‰è¨­å®š\nSEED = 42\nnp.random.seed(SEED)'),
        nbf.v4.new_markdown_cell('## 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿'),
        nbf.v4.new_code_cell('# å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\nX_train = pd.read_csv("../data/processed/X_train_processed.csv")\nX_test = pd.read_csv("../data/processed/X_test_processed.csv")\ny_train = pd.read_csv("../data/processed/y_train.csv").iloc[:, 0]\n\nprint(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_train.shape}")\nprint(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_test.shape}")\nprint(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå½¢çŠ¶: {y_train.shape}")'),
        nbf.v4.new_markdown_cell('## 3. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰'),
        nbf.v4.new_code_cell('# LightGBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«\nfrom sklearn.model_selection import KFold\n\n# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n\n# LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\nlgb_params = {\n    "objective": "regression",\n    "metric": "rmse",\n    "boosting_type": "gbdt",\n    "num_leaves": 31,\n    "learning_rate": 0.05,\n    "feature_fraction": 0.9,\n    "bagging_fraction": 0.8,\n    "bagging_freq": 5,\n    "verbose": -1,\n    "random_state": SEED\n}\n\n# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ\noof_predictions = np.zeros(len(X_train))\ntest_predictions = np.zeros(len(X_test))\ncv_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n    print(f"Fold {fold + 1}")\n    \n    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n    \n    # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ\n    train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n    val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n    \n    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´\n    model = lgb.train(\n        lgb_params,\n        train_data,\n        valid_sets=[val_data],\n        num_boost_round=1000,\n        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n    )\n    \n    # äºˆæ¸¬\n    val_pred = model.predict(X_val_fold, num_iteration=model.best_iteration)\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    \n    oof_predictions[val_idx] = val_pred\n    test_predictions += test_pred / 5\n    \n    # ã‚¹ã‚³ã‚¢è¨ˆç®—\n    fold_score = np.sqrt(mean_squared_error(y_val_fold, val_pred))\n    cv_scores.append(fold_score)\n    print(f"Fold {fold + 1} RMSE: {fold_score:.6f}")\n\n# å…¨ä½“ã®ã‚¹ã‚³ã‚¢\noverall_score = np.sqrt(mean_squared_error(y_train, oof_predictions))\nprint(f"\\nOverall CV RMSE: {overall_score:.6f}")\nprint(f"CV RMSE: {np.mean(cv_scores):.6f} Â± {np.std(cv_scores):.6f}")'),
        nbf.v4.new_markdown_cell('## 4. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ'),
        nbf.v4.new_code_cell('# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\nsubmission = pd.DataFrame({\n    "id": range(len(test_predictions)),  # é©å®œIDã‚«ãƒ©ãƒ ã‚’èª¿æ•´\n    "target": test_predictions  # é©å®œã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ åã‚’èª¿æ•´\n})\n\nos.makedirs("../submissions", exist_ok=True)\nsubmission.to_csv("../submissions/baseline_submission.csv", index=False)\nprint("æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: ../submissions/baseline_submission.csv")\nprint(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«å½¢çŠ¶: {submission.shape}")\ndisplay(submission.head())')
    ]
    
    with open('shared/notebooks/03_modeling_template.ipynb', 'w') as f:
        nbf.write(nb, f)

def create_inference_notebook():
    """æ¨è«–ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆ"""
    nb = nbf.v4.new_notebook()
    nb.cells = [
        nbf.v4.new_markdown_cell('# æ¨è«–ãƒ»æå‡º ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n\nã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Kaggleç«¶æŠ€ã«ãŠã‘ã‚‹æ¨è«–ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã™ã€‚'),
        nbf.v4.new_markdown_cell('## 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ'),
        nbf.v4.new_code_cell('import pandas as pd\nimport numpy as np\nimport joblib\nimport os\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings("ignore")\n\n# è¨­å®š\npd.set_option("display.max_columns", None)'),
        nbf.v4.new_markdown_cell('## 2. ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿'),
        nbf.v4.new_code_cell('# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\nX_test = pd.read_csv("../data/processed/X_test_processed.csv")\nprint(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_test.shape}")\n\n# å…ƒã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆIDã‚«ãƒ©ãƒ ç”¨ï¼‰\ntest_original = pd.read_csv("../data/raw/test.csv")\nprint(f"å…ƒã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_original.shape}")\n\n# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆä¾‹ï¼šjoblibå½¢å¼ï¼‰\nmodel_dir = "../models/"\nif os.path.exists(model_dir):\n    model_files = [f for f in os.listdir(model_dir) if f.endswith(".joblib")]\n    print(f"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {model_files}")\n    \n    # è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç”¨ï¼‰\n    models = {}\n    for model_file in model_files:\n        model_name = model_file.replace(".joblib", "")\n        models[model_name] = joblib.load(os.path.join(model_dir, model_file))\n        print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_name}")\nelse:\n    print("ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")'),
        nbf.v4.new_markdown_cell('## 3. æ¨è«–å®Ÿè¡Œ'),
        nbf.v4.new_code_cell('# æ¨è«–å®Ÿè¡Œï¼ˆãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼‰\nif "models" in locals() and models:\n    if len(models) == 1:\n        # å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ã®æ¨è«–\n        model_name = list(models.keys())[0]\n        model = models[model_name]\n        \n        print(f"å˜ä¸€ãƒ¢ãƒ‡ãƒ«æ¨è«–: {model_name}")\n        predictions = model.predict(X_test)\n        \n    else:\n        # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n        print("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è«–")\n        all_predictions = []\n        \n        for model_name, model in models.items():\n            pred = model.predict(X_test)\n            all_predictions.append(pred)\n            print(f"{model_name}ã®æ¨è«–å®Œäº†")\n        \n        # å¹³å‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n        predictions = np.mean(all_predictions, axis=0)\n        print("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Œäº†ï¼ˆå¹³å‡ï¼‰")\n    \n    print(f"äºˆæ¸¬å€¤ã®å½¢çŠ¶: {predictions.shape}")\n    print(f"äºˆæ¸¬å€¤ã®çµ±è¨ˆ: min={predictions.min():.6f}, max={predictions.max():.6f}, mean={predictions.mean():.6f}")\nelse:\n    print("ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ€ãƒŸãƒ¼ã®äºˆæ¸¬å€¤ã‚’ä½œæˆã—ã¾ã™ã€‚")\n    predictions = np.random.random(len(X_test))'),
        nbf.v4.new_markdown_cell('## 4. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ'),
        nbf.v4.new_code_cell('# IDã‚«ãƒ©ãƒ ã®ç‰¹å®šï¼ˆé©å®œèª¿æ•´ï¼‰\nid_column = "id"  # ç«¶æŠ€ã«å¿œã˜ã¦å¤‰æ›´\ntarget_column = "target"  # ç«¶æŠ€ã«å¿œã˜ã¦å¤‰æ›´\n\n# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\nif id_column in test_original.columns:\n    submission = pd.DataFrame({\n        id_column: test_original[id_column],\n        target_column: predictions\n    })\nelse:\n    # IDã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯é€£ç•ªã§ä½œæˆ\n    submission = pd.DataFrame({\n        id_column: range(len(predictions)),\n        target_column: predictions\n    })\n\nprint(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«å½¢çŠ¶: {submission.shape}")\nprint("æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®5è¡Œ:")\ndisplay(submission.head())\n\nprint("\\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¾Œã®5è¡Œ:")\ndisplay(submission.tail())'),
        nbf.v4.new_markdown_cell('## 5. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜'),
        nbf.v4.new_code_cell('# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å\ntimestamp = datetime.now().strftime("%Y%m%d_%H%M%S")\nfilename = f"submission_{timestamp}.csv"\nfilepath = f"../submissions/{filename}"\n\n# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\nos.makedirs("../submissions", exist_ok=True)\n\n# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜\nsubmission.to_csv(filepath, index=False)\nprint(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")\n\n# æœ€æ–°ã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚‚ã‚³ãƒ”ãƒ¼\nlatest_filepath = "../submissions/latest_submission.csv"\nsubmission.to_csv(latest_filepath, index=False)\nprint(f"æœ€æ–°æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {latest_filepath}")')
    ]
    
    with open('shared/notebooks/04_inference_template.ipynb', 'w') as f:
        nbf.write(nb, f)

if __name__ == "__main__":
    print("Jupyter notebook templatesä½œæˆä¸­...")
    create_eda_notebook()
    print("âœ… 01_EDA_template.ipynb ä½œæˆå®Œäº†")
    
    create_feature_engineering_notebook()
    print("âœ… 02_feature_engineering_template.ipynb ä½œæˆå®Œäº†")
    
    create_modeling_notebook()
    print("âœ… 03_modeling_template.ipynb ä½œæˆå®Œäº†")
    
    create_inference_notebook()
    print("âœ… 04_inference_template.ipynb ä½œæˆå®Œäº†")
    
    print("ğŸ‰ å…¨ã¦ã®Jupyter notebook templatesä½œæˆå®Œäº†!")
